Some common errors you may get, especially as you create more complex applications using
3rd-party libraries, are “task not serializable” errors. These typically crop up on complex
tasks referencing member variables of the class defining the closures.
In this example we have a very basic helper class that doesn’t really do much. We reference
it in a map operation. If we try to run an action on the output RDD we’d get an error.
The problem is that “MyHelper” doesn’t implement Serializable. So if we extend our
class with Serializable and try to run it again it will work properly.
What if “MyHelper” in turn is also referencing another 3rd party library as a member variable
which is not serializable? We may not want (or be able to) change the code to make it
serializable. Also, because of the cost of sending large objects as part of our it would
be better not to serialize the whole object graph.
What other option do we have? If we mark the member variable transient and
lazy, it will not get serialized but will still be instantiated locally within each
task and our code will run as expected, without incurring the cost of serializing a large
task or lots of code changes. Here we have another example, this time of
a full Scala app. We have a local class “MyOtherHelper” used in a map operation. In its current form
this example will fail, even though the helper class is serializable. Why?
Because “helper1” is a member variable of MySparkApp,
and so Spark tries to serialize that instance as well.
We just could make MySparkApp serializable, but is it really a good idea to ship our whole
app to every worker? A better option is to make a local reference
to “helper1”. This way we’re keeping our task as slim as possible an minimizing
serialization issues over large object graphs. As we discussed earlier, stages are defined
by shuffle dependencies. Here we have a graph of 2 RDDs being joined,
then a reduceByKey, and finally a saveAsTextFile. Here is the RDD lineage from calling to debug
string. The debug string clearly highlights the stage
boundaries. Stage one and two are the original RDDs being
filtered, mapped and keyed in preparation for the join.
The join requires a shuffle, so there is a stage boundary there. The third stage is the
reduce and save after the join. Spark has a featured called speculative execution
for handling slow tasks. As stages are running slow tasks will be re-launched
as necessary. Speculative execution is disabled by default.
We can enable it by setting the spark.speculation parameter to true.
“Slow” is a configurable value set by spark.speculation.multiplier. This defines
how many times slower a task is than the median to be considered for speculation.
We also need a way to handle failing tasks. Tasks could fail either due to memory issues,
underlying hardware issues, networking problems, etc.
By default Spark will retry a task 3 times before failing a stage, which can be changed
with the spark.task.maxFailures parameter as needed.
For example you may want to fail fast versus running a long task multiple times to find
out there’s an issue on your cluster. An important issue to be aware of is any operation
that has side-effects, such as outputting data in a for each call.
If our task fails and gets re-run it may very well try to output that data again.
If you’re using custom code to write out to another system be aware that you may need
to ensure the operation is idempotent. Most of what we’ve looked at so far as been
a single-user working in the Spark shell or a single app being submitted working on a
single task. By default, Spark uses a basic FIFO scheduler. It works well for single-user
apps where each job gets as many resources it needs.
The SparkContext is fully thread-safe so within a Spark app we can actually submit jobs from
multiple threads. If multiple jobs are submitted concurrently they could run in parallel if
the first job doesn’t need all the cluster resources.
However, if that’s not the case you may have a single large job that backs up all
other requests until it’s finished. For multi-user environments such as Zeppelin,
you'll want to use the Fair scheduler, which can be set with the spark.schedule.mode parameter.
By default the fair scheduler divvies up the cluster resources in a round-robin fashion,
giving equal shares of resources. So even if you have a very large job it won’t stop
a small, short, job from starting and running. We can also define pools, that have custom
attributes such as weights and minimum shares of the the cluster resources. Pools can have
their own scheduler. Pools are useful for defining priority queues
(like for a certain user or group) or by giving each user a pool which is internally
a just a FIFO queue. This is the model Zeppelin uses to allow multiple users to work on the
Spark cluster at the same time. Configuring custom Fair Scheduler pools involves
first defining an XML file with the pools and their config properties. Then in your
Spark app, or config settings, you need to set the spark.scheduler.allocation.file setting,
pointing to that XML file. Submitting a job for a specific pool is done
by setting a local property on the SparkContext, spark.scheduler.pool. This is a thread local
property so if you spawn multiple threads on your driver each will have to set this
value. To clear the value, just set it to null.
Having completed this lesson, you should be able to understand how Spark generates RDDs,
manage partitions to improve RDD performance, understand what makes an RDD resilient, understand
how RDDs are broken into jobs and stages, and serialize tasks.
Proceed to exercise 2 and the next lesson.