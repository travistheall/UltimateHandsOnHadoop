Lesson 4: Cashing and Serialization
This course was developed in collaboration with MetiStream and IBM Analytics.
After completing this lesson you should be able to understand how and when to cache RDDs,
understand storage levels and their uses, optimize memory usage with serialization options,
and share RDDs with Tachyon.
Spark is very well suited to iterative analysis because of how it distributes and executes
tasks across the cluster and the fact that it minimizes disk I/O compared to frameworks
such as MapReduce. However, sometimes we may need some additional performance gains while
dealing with the same RDD over numerous iterations. What Spark is most known for is it’s use
of memory caching. It’s important to understand when and how we should persist RDDs. Persistence
doesn’t necessarily have to be in-memory only. We can use a combination if we know
the data is too large to fit in memory or we could choose to save entirely to disk if
we’re more concerned with losing results of an expensive operation. Persisting to disk
would allow us to reconstitute the RDD from disk in the event a partition is lost, instead
of re-computing all the expensive operations for the lost partitions.
Ideally you want to persist after any pruning, filtering, and other transformations needed
for downstream processing. An example is loading a file, parsing it, and partitioning it by
a key. It wouldn’t do much good to simply cache the root RDD if we would have to incur
the filter, map, and shuffle again each time we re-use that RDD. Instead, we’d be better
off caching after the partitionBy. When you no longer need the persisted RDD
simply call un-persist. Spark will also use a Least Recently Used (LRU) algorithm as needed
to make room for new RDDs. In this RDD lineage we’re joining 2 RDDs,
one which is filtered. Then we have two branching operations: a reduceByKey and a mapValues.
It would be a good idea to persist after the join, when the RDD is prepped and ready for
the reduce and map actions downstream. These are the different storage levels available,
from the Apache Spark programming guide. MEMORY_ONLY is the default. You can call the shorthand
cache function to persist with the default. Note how the different levels handle overflow.
With memory only, if the whole RDD can't fit in memory, some of the partitions will have
to be recomputed one the fly. The DISK options write the overflow to disk instead.
You can also replicate persisted partitions, or store them in Tachyon, a way to "share"
RDDs that we will talk about shortly. While persisting RDDs might help us save time
re-computing partitions it does come at a cost. That is the space it takes to keep all
those objects in memory. Keeping records in memory in a raw state takes the most space.
To help save on space we can serialize. The records of an RDD will be stored as one large
byte array. You’ll incur some CPU usage to deserialize the data. However it has the
added benefit of helping with garbage collection as you’ll be storing 1 object (the byte
array) versus many small objects for each record.
The default serializer is the Java serializer, or pickle if you’re using python.
Another option for Java and Scala users is the Kryo serializer, which is more efficient
at the cost of some potential compatibility or configuration issues.
You can also compress serialized in-memory RDDs at the cost of decompressing
As a rule of thumb optimize storage by using primitive types instead of Java or Scala collections.
Also try to avoid nested classes with many small objects as that will incur large garbage
collection overhead.